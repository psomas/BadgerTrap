commit cb9f4271bc1ead6bcb163d8b83e00ce7c39b08b2
Author: Stratos Psomadakis <774566+psomas@users.noreply.github.com>
Date:   Tue Jul 7 16:40:43 2020 +0300

    Badgertrap v4.19.60 patch

diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl
index f0b1709a5..b3ae150b8 100644
--- a/arch/x86/entry/syscalls/syscall_64.tbl
+++ b/arch/x86/entry/syscalls/syscall_64.tbl
@@ -343,6 +343,7 @@
 332	common	statx			__x64_sys_statx
 333	common	io_pgetevents		__x64_sys_io_pgetevents
 334	common	rseq			__x64_sys_rseq
+335	64	init_badger_trap	__x64_sys_init_badger_trap
 
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
diff --git a/arch/x86/include/asm/page_64_types.h b/arch/x86/include/asm/page_64_types.h
index 0b6352aab..776b1c4a8 100644
--- a/arch/x86/include/asm/page_64_types.h
+++ b/arch/x86/include/asm/page_64_types.h
@@ -56,7 +56,7 @@
 
 /* See Documentation/x86/x86_64/mm.txt for a description of the memory map. */
 
-#define __PHYSICAL_MASK_SHIFT	52
+#define __PHYSICAL_MASK_SHIFT	46
 
 #ifdef CONFIG_X86_5LEVEL
 #define __VIRTUAL_MASK_SHIFT	(pgtable_l5_enabled() ? 56 : 47)
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 9d9765e4d..d7842ea7d 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -1168,8 +1168,12 @@ access_error(unsigned long error_code, struct vm_area_struct *vma)
 	}
 
 	/* read, present: */
-	if (unlikely(error_code & X86_PF_PROT))
-		return 1;
+	if (unlikely(error_code & X86_PF_PROT)) {
+		if((error_code & X86_PF_RSVD) && current->mm->badger_trap_en==1)
+			return 0;
+		else
+			return 1;
+	}
 
 	/* read, not present: */
 	if (unlikely(!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE))))
@@ -1263,7 +1267,7 @@ __do_page_fault(struct pt_regs *regs, unsigned long error_code,
 	if (unlikely(kprobes_fault(regs)))
 		return;
 
-	if (unlikely(error_code & X86_PF_RSVD))
+	if (unlikely(error_code & X86_PF_RSVD && current->mm->badger_trap_en==0))
 		pgtable_bad(regs, error_code, address);
 
 	if (unlikely(smap_violation(error_code, regs))) {
diff --git a/fs/exec.c b/fs/exec.c
index 433b12576..3f4f88810 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -72,6 +72,9 @@
 
 #include <trace/events/sched.h>
 
+extern int is_badger_trap_process(const char* proc_name);
+extern void badger_trap_init(struct mm_struct *mm);
+
 int suid_dumpable = 0;
 
 static LIST_HEAD(formats);
@@ -1370,6 +1373,19 @@ void setup_new_exec(struct linux_binprm * bprm)
 	perf_event_exec();
 	__set_task_comm(current, kbasename(bprm->filename), true);
 
+	/* Check if we need to enable badger trap for this process*/
+	if(is_badger_trap_process(current->comm))
+	{
+		current->mm->badger_trap_en = 1;
+		badger_trap_init(current->mm);
+	}
+
+	if(current && current->real_parent && current->real_parent != current && current->real_parent->mm && current->real_parent->mm->badger_trap_en)
+	{
+		current->mm->badger_trap_en = 1;
+		badger_trap_init(current->mm);
+	}
+
 	/* Set the new mm task size. We have to do that late because it may
 	 * depend on TIF_32BIT which is only updated in flush_thread() on
 	 * some architectures like powerpc
diff --git a/include/linux/badger_trap.h b/include/linux/badger_trap.h
new file mode 100644
index 000000000..fb6b2aa01
--- /dev/null
+++ b/include/linux/badger_trap.h
@@ -0,0 +1,17 @@
+#ifndef _LINUX_BADGER_TRAP_H
+#define _LINUX_BADGER_TRAP_H
+
+#define MAX_NAME_LEN	16
+#define PTE_RESERVED_MASK	(_AT(pteval_t, 1) << 51)
+
+char badger_trap_process[CONFIG_NR_CPUS][MAX_NAME_LEN];
+
+int is_badger_trap_process(const char* proc_name);
+inline pte_t pte_mkreserve(pte_t pte);
+inline pte_t pte_unreserve(pte_t pte);
+inline int is_pte_reserved(pte_t pte);
+inline pmd_t pmd_mkreserve(pmd_t pmd);
+inline pmd_t pmd_unreserve(pmd_t pmd);
+inline int is_pmd_reserved(pmd_t pmd);
+void badger_trap_init(struct mm_struct *mm);
+#endif /* _LINUX_BADGER_TRAP_H */
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 5ed8f6292..5950bc03b 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -493,6 +493,14 @@ struct mm_struct {
 #endif
 	} __randomize_layout;
 
+    /*
+     *  Variables for Badger Trap
+     */
+    unsigned int badger_trap_en;
+    unsigned long total_dtlb_misses;
+    unsigned long total_dtlb_4k_misses;
+    unsigned long total_dtlb_hugetlb_misses;
+
 	/*
 	 * The mm_cpumask needs to be at the end of mm_struct, because it
 	 * is dynamically sized based on nr_cpu_ids.
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5dc024e28..03837ba0f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1193,6 +1193,10 @@ struct task_struct {
 	void				*security;
 #endif
 
+	unsigned long total_dtlb_misses;
+	unsigned long total_dtlb_4k_misses;
+	unsigned long total_dtlb_hugetlb_misses;
+
 	/*
 	 * New fields for task_struct should be added above here, so that
 	 * they are included in the randomized portion of task_struct.
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index 2ff814c92..69bdad6a8 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -906,6 +906,7 @@ asmlinkage long sys_statx(int dfd, const char __user *path, unsigned flags,
 			  unsigned mask, struct statx __user *buffer);
 asmlinkage long sys_rseq(struct rseq __user *rseq, uint32_t rseq_len,
 			 int flags, uint32_t sig);
+asmlinkage long sys_init_badger_trap(const char __user** process_name, unsigned long num_procs, int options);
 
 /*
  * Architecture-specific system calls
diff --git a/kernel/exit.c b/kernel/exit.c
index 5c0964dc8..463dc5d1f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -775,10 +775,44 @@ void __noreturn do_exit(long code)
 {
 	struct task_struct *tsk = current;
 	int group_dead;
+	static DEFINE_MUTEX(result_mutex);
 
 	profile_task_exit(tsk);
 	kcov_task_exit(tsk);
 
+        /*
+	 * Statistics for Badger Trap
+ 	 */
+        if(current->mm && current->mm->badger_trap_en == 1)
+        {
+		mutex_lock(&result_mutex);
+		current->mm->total_dtlb_misses+= current->total_dtlb_misses;
+		current->mm->total_dtlb_4k_misses+= current->total_dtlb_4k_misses;
+		current->mm->total_dtlb_hugetlb_misses+= current->total_dtlb_hugetlb_misses;
+		mutex_unlock(&result_mutex);
+	}
+
+	if(current->mm && current->mm->badger_trap_en == 1 && current->tgid == current->pid)
+	{
+		if(current->real_parent->mm->badger_trap_en == 1)
+		{
+			mutex_lock(&result_mutex);
+			current->real_parent->mm->total_dtlb_misses+=current->mm->total_dtlb_misses;
+			current->real_parent->mm->total_dtlb_4k_misses+=current->mm->total_dtlb_4k_misses;
+			current->real_parent->mm->total_dtlb_hugetlb_misses+=current->mm->total_dtlb_hugetlb_misses;
+			mutex_unlock(&result_mutex);
+		}
+		else
+		{
+			printk("===================================\n");
+			printk("Statistics for Process %s\n",current->comm);
+			printk("DTLB miss detected %lu\n",current->mm->total_dtlb_misses);
+			printk("DTLB miss for 4KB page detected %lu\n",current->mm->total_dtlb_4k_misses);
+			printk("DTLB miss for hugepage detected %lu\n",current->mm->total_dtlb_hugetlb_misses);
+			printk("===================================\n");
+		}
+	}
+
 	WARN_ON(blk_needs_flush_plug(tsk));
 
 	if (unlikely(in_interrupt()))
diff --git a/mm/Makefile b/mm/Makefile
index 26ef77a38..184cf08e7 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -39,7 +39,7 @@ obj-y			:= filemap.o mempool.o oom_kill.o fadvise.o \
 			   mm_init.o mmu_context.o percpu.o slab_common.o \
 			   compaction.o vmacache.o \
 			   interval_tree.o list_lru.o workingset.o \
-			   debug.o $(mmu-y)
+			   debug.o badger_trap.o $(mmu-y)
 
 obj-y += init-mm.o
 
diff --git a/mm/badger_trap.c b/mm/badger_trap.c
new file mode 100644
index 000000000..160d77341
--- /dev/null
+++ b/mm/badger_trap.c
@@ -0,0 +1,213 @@
+#include <asm/pgalloc.h>
+#include <asm/uaccess.h>
+#include <asm/pgtable.h>
+#include <linux/badger_trap.h>
+#include <linux/syscalls.h>
+#include <linux/hugetlb.h>
+#include <linux/kernel.h>
+
+
+/*
+ * This syscall is generic way of setting up badger trap. 
+ * There are three options to start badger trap.
+ * (1) 	option > 0: provide all process names with number of processes.
+ * 	This will mark the process names for badger trap to start when any
+ * 	process with names specified will start.
+ *
+ * (2) 	option == 0: starts badger trap for the process calling the syscall itself.
+ *  	This requires binary to be updated for the workload to call badger trap. This
+ *  	option is useful when you want to skip the warmup phase of the program. You can 
+ *  	introduce the syscall in the program to invoke badger trap after that phase.
+ *
+ * (3) 	option < 0: provide all pid with number of processes. This will start badger
+ *  	trap for all pids provided immidiately.
+ *
+ *  Note: 	(1) will allow all the child processes to be marked for badger trap when
+ *  		forked from a badger trap process.
+
+ *		(2) and (3) will not mark the already spawned child processes for badger 
+ *		trap when you mark the parent process for badger trap on the fly. But (2) and (3) 
+ *		will mark all child spwaned from the parent process adter being marked for badger trap. 
+ */
+SYSCALL_DEFINE3(init_badger_trap, const char __user**, process_name, unsigned long, num_procs, int, option)
+{
+	unsigned int i;
+	char *temp;
+	unsigned long ret=0;
+	char proc[MAX_NAME_LEN];
+	struct task_struct * tsk;
+	unsigned long pid;
+
+	if(option > 0)
+	{
+		for(i=0; i<CONFIG_NR_CPUS; i++)
+		{
+			if(i<num_procs) {
+				stac();
+				ret = strncpy_from_user(proc, process_name[i], MAX_NAME_LEN);
+				clac();
+			}
+			else
+				temp = strncpy(proc,"",MAX_NAME_LEN);
+			temp = strncpy(badger_trap_process[i], proc, MAX_NAME_LEN-1);
+		}
+	}
+
+	// All other inputs ignored
+	if(option == 0)
+	{
+		current->mm->badger_trap_en = 1;
+		badger_trap_init(current->mm);
+	}
+
+	if(option < 0)
+	{
+		for(i=0; i<CONFIG_NR_CPUS; i++)
+		{
+			if(i<num_procs)
+			{
+        stac();
+				ret = kstrtoul(process_name[i],10,&pid);
+        clac();
+				if(ret == 0)
+				{
+					tsk = find_task_by_vpid(pid);
+					tsk->mm->badger_trap_en = 1;
+					badger_trap_init(tsk->mm);
+				}
+			}
+		}
+	}
+
+	return 0;
+}
+
+/*
+ * This function checks whether a process name provided matches from the list
+ * of process names stored to be marked for badger trap.
+ */
+int is_badger_trap_process(const char* proc_name)
+{
+	unsigned int i;
+	for(i=0; i<CONFIG_NR_CPUS; i++)
+	{
+		if(!strncmp(proc_name,badger_trap_process[i],MAX_NAME_LEN))
+			return 1;
+	}
+	return 0;
+}
+
+/*
+ * Helper functions to manipulate all the TLB entries for reservation.
+ */
+inline pte_t pte_mkreserve(pte_t pte)
+{
+        return pte_set_flags(pte, PTE_RESERVED_MASK);
+}
+
+inline pte_t pte_unreserve(pte_t pte)
+{
+        return pte_clear_flags(pte, PTE_RESERVED_MASK);
+}
+
+inline int is_pte_reserved(pte_t pte)
+{
+        if(native_pte_val(pte) & PTE_RESERVED_MASK)
+                return 1;
+        else
+                return 0;
+}
+
+inline pmd_t pmd_mkreserve(pmd_t pmd)
+{
+        return pmd_set_flags(pmd, PTE_RESERVED_MASK);
+}
+
+inline pmd_t pmd_unreserve(pmd_t pmd)
+{
+        return pmd_clear_flags(pmd, PTE_RESERVED_MASK);
+}
+
+inline int is_pmd_reserved(pmd_t pmd)
+{
+        if(native_pmd_val(pmd) & PTE_RESERVED_MASK)
+                return 1;
+        else
+                return 0;
+}
+
+/*
+ * This function walks the page table of the process being marked for badger trap
+ * This helps in finding all the PTEs that are to be marked as reserved. This is 
+ * espicially useful to start badger trap on the fly using (2) and (3). If we do not
+ * call this function, when starting badger trap for any process, we may miss some TLB 
+ * misses from being tracked which may not be desierable.
+ *
+ * Note: This function takes care of transparent hugepages and hugepages in general.
+ */
+void badger_trap_init(struct mm_struct *mm)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte;
+	pte_t *page_table;
+	spinlock_t *ptl;
+	unsigned long address;
+	unsigned long i,j,k,l;
+	unsigned long user = 0;
+	unsigned long mask = _PAGE_USER | _PAGE_PRESENT;
+	struct vm_area_struct *vma;
+	pgd_t *base = mm->pgd;
+	for(i=0; i<PTRS_PER_PGD; i++)
+	{
+		pgd = base + i;
+		if((pgd_flags(*pgd) & mask) != mask)
+			continue;
+		for(j=0; j<PTRS_PER_PUD; j++)
+		{
+			pud = (pud_t *)pgd_page_vaddr(*pgd) + j;
+			if((pud_flags(*pud) & mask) != mask)
+                        	continue;
+			address = (i<<PGDIR_SHIFT) + (j<<PUD_SHIFT);
+			if(vma && pud_huge(*pud) && is_vm_hugetlb_page(vma))
+			{
+				spin_lock(&mm->page_table_lock);
+				page_table = huge_pte_offset(mm, address, PUD_SIZE); /* FIXME: PUD_SIZE -> huge_page_size(h) */
+				*page_table = pte_mkreserve(*page_table);
+				spin_unlock(&mm->page_table_lock);
+				continue;
+			}
+			for(k=0; k<PTRS_PER_PMD; k++)
+			{
+				pmd = (pmd_t *)pud_page_vaddr(*pud) + k;
+				if((pmd_flags(*pmd) & mask) != mask)
+					continue;
+				address = (i<<PGDIR_SHIFT) + (j<<PUD_SHIFT) + (k<<PMD_SHIFT);
+				vma = find_vma(mm, address);
+				if(vma && pmd_huge(*pmd) && (transparent_hugepage_enabled(vma)||is_vm_hugetlb_page(vma)))
+				{
+					spin_lock(&mm->page_table_lock);
+					*pmd = pmd_mkreserve(*pmd);
+					spin_unlock(&mm->page_table_lock);
+					continue;
+				}
+				for(l=0; l<PTRS_PER_PTE; l++)
+				{
+					pte = (pte_t *)pmd_page_vaddr(*pmd) + l;
+					if((pte_flags(*pte) & mask) != mask)
+						continue;
+					address = (i<<PGDIR_SHIFT) + (j<<PUD_SHIFT) + (k<<PMD_SHIFT) + (l<<PAGE_SHIFT);
+					vma = find_vma(mm, address);
+					if(vma)
+					{
+						page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
+						*pte = pte_mkreserve(*pte);
+						pte_unmap_unlock(page_table, ptl);
+					}
+					user++;
+				}
+			}
+		}
+	}
+}
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 6fad1864b..95c4225da 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -38,6 +38,9 @@
 #include <asm/pgalloc.h>
 #include "internal.h"
 
+extern inline pmd_t pmd_mkreserve(pmd_t pmd);
+extern inline pte_t pte_mkreserve(pte_t pte);
+
 /*
  * By default, transparent hugepage support is disabled in order to avoid
  * risking an increased memory footprint for applications that are not
@@ -601,6 +604,13 @@ static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf,
 		mem_cgroup_commit_charge(page, memcg, false, true);
 		lru_cache_add_active_or_unevictable(page, vma);
 		pgtable_trans_huge_deposit(vma->vm_mm, vmf->pmd, pgtable);
+
+		/* Make the page table entry as reserved for TLB miss tracking */
+		if(vma->vm_mm && (vma->vm_mm->badger_trap_en==1) && (!(vmf->flags & FAULT_FLAG_INSTRUCTION)))
+		{
+			entry = pmd_mkreserve(entry);
+		}
+
 		set_pmd_at(vma->vm_mm, haddr, vmf->pmd, entry);
 		add_mm_counter(vma->vm_mm, MM_ANONPAGES, HPAGE_PMD_NR);
 		mm_inc_nr_ptes(vma->vm_mm);
@@ -658,6 +668,16 @@ static bool set_huge_zero_page(pgtable_t pgtable, struct mm_struct *mm,
 	entry = pmd_mkhuge(entry);
 	if (pgtable)
 		pgtable_trans_huge_deposit(mm, pmd, pgtable);
+
+	/* Make the page table entry as reserved for TLB miss tracking 
+	 * No need to worry for zero page with instruction faults.
+	 * Instruction faults will never reach here.
+	 */
+	if(mm && (mm->badger_trap_en==1))
+	{
+		entry = pmd_mkreserve(entry);
+	}		
+
 	set_pmd_at(mm, haddr, pmd, entry);
 	mm_inc_nr_ptes(mm);
 	return true;
@@ -1241,6 +1261,13 @@ static vm_fault_t do_huge_pmd_wp_page_fallback(struct vm_fault *vmf,
 		lru_cache_add_active_or_unevictable(pages[i], vma);
 		vmf->pte = pte_offset_map(&_pmd, haddr);
 		VM_BUG_ON(!pte_none(*vmf->pte));
+
+		/* Make the page table entry as reserved for TLB miss tracking */
+		if(vma->vm_mm && (vma->vm_mm->badger_trap_en==1) && (!(vmf->flags & FAULT_FLAG_INSTRUCTION)))
+		{
+			entry = pte_mkreserve(entry);
+		}
+
 		set_pte_at(vma->vm_mm, haddr, vmf->pte, entry);
 		pte_unmap(vmf->pte);
 	}
@@ -1391,6 +1418,13 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
 		entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);
 		pmdp_huge_clear_flush_notify(vma, haddr, vmf->pmd);
 		page_add_new_anon_rmap(new_page, vma, haddr, true);
+
+		/* Make the page table entry as reserved for TLB miss tracking */
+		if(vma->vm_mm && (vma->vm_mm->badger_trap_en==2) && (!(vmf->flags & FAULT_FLAG_INSTRUCTION)))
+		{
+			entry = pmd_mkreserve(entry);
+		}
+
 		mem_cgroup_commit_charge(new_page, memcg, false, true);
 		lru_cache_add_active_or_unevictable(new_page, vma);
 		set_pmd_at(vma->vm_mm, haddr, vmf->pmd, entry);
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 57053affa..150d01624 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -55,6 +55,10 @@ static unsigned long __initdata default_hstate_max_huge_pages;
 static unsigned long __initdata default_hstate_size;
 static bool __initdata parsed_valid_hugepagesz = true;
 
+extern inline pte_t pte_mkreserve(pte_t pte);
+extern inline pte_t pte_unreserve(pte_t pte);
+extern inline int is_pte_reserved(pte_t pte);
+
 /*
  * Protects updates to hugepage_freelists, hugepage_activelist, nr_huge_pages,
  * free_huge_pages, and surplus_huge_pages.
@@ -3561,7 +3565,7 @@ static void unmap_ref_private(struct mm_struct *mm, struct vm_area_struct *vma,
  */
 static vm_fault_t hugetlb_cow(struct mm_struct *mm, struct vm_area_struct *vma,
 		       unsigned long address, pte_t *ptep,
-		       struct page *pagecache_page, spinlock_t *ptl)
+		       struct page *pagecache_page, spinlock_t *ptl, unsigned int flags)
 {
 	pte_t pte;
 	struct hstate *h = hstate_vma(vma);
@@ -3571,6 +3575,7 @@ static vm_fault_t hugetlb_cow(struct mm_struct *mm, struct vm_area_struct *vma,
 	unsigned long mmun_start;	/* For mmu_notifiers */
 	unsigned long mmun_end;		/* For mmu_notifiers */
 	unsigned long haddr = address & huge_page_mask(h);
+	pte_t new_entry;
 
 	pte = huge_ptep_get(ptep);
 	old_page = pte_page(pte);
@@ -3664,8 +3669,16 @@ static vm_fault_t hugetlb_cow(struct mm_struct *mm, struct vm_area_struct *vma,
 		/* Break COW */
 		huge_ptep_clear_flush(vma, haddr, ptep);
 		mmu_notifier_invalidate_range(mm, mmun_start, mmun_end);
-		set_huge_pte_at(mm, haddr, ptep,
-				make_huge_pte(vma, new_page, 1));
+
+		new_entry = make_huge_pte(vma, new_page, 1);
+		set_huge_pte_at(mm, address, ptep, new_entry);
+
+		/* Make the page table entry as reserved for TLB miss tracking */
+                if(mm && (mm->badger_trap_en==1) && (!(flags & FAULT_FLAG_INSTRUCTION)))
+                {
+                        *ptep = pte_mkreserve(*ptep);
+                }
+
 		page_remove_rmap(old_page, true);
 		hugepage_add_new_anon_rmap(new_page, vma, haddr);
 		set_page_huge_active(new_page);
@@ -3876,12 +3889,19 @@ static vm_fault_t hugetlb_no_page(struct mm_struct *mm,
 		page_dup_rmap(page, true);
 	new_pte = make_huge_pte(vma, page, ((vma->vm_flags & VM_WRITE)
 				&& (vma->vm_flags & VM_SHARED)));
+
+	/* Make the page table entry as reserved for TLB miss tracking */
+	if(mm && (mm->badger_trap_en==1) && (!(flags & FAULT_FLAG_INSTRUCTION)))
+        {
+		new_pte = pte_mkreserve(new_pte);
+	}
+
 	set_huge_pte_at(mm, haddr, ptep, new_pte);
 
 	hugetlb_count_add(pages_per_huge_page(h), mm);
 	if ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {
 		/* Optimization, do the COW without a second fault */
-		ret = hugetlb_cow(mm, vma, address, ptep, page, ptl);
+		ret = hugetlb_cow(mm, vma, address, ptep, page, ptl, flags);
 	}
 
 	spin_unlock(ptl);
@@ -3933,6 +3953,49 @@ u32 hugetlb_fault_mutex_hash(struct hstate *h, struct address_space *mapping,
 }
 #endif
 
+static int hugetlb_fake_fault(struct mm_struct *mm, struct vm_area_struct *vma,
+                        unsigned long address, pte_t *page_table, unsigned int flags)
+{
+        unsigned long *touch_page_addr;
+        unsigned long touched;
+        unsigned long ret;
+        static unsigned int consecutive = 0;
+        static unsigned long prev_address = 0;
+
+        if(address == prev_address)
+                consecutive++;
+        else
+        {
+                consecutive = 0;
+                prev_address = address;
+        }
+
+        if(consecutive > 1)
+        {
+                *page_table = pte_unreserve(*page_table);
+                return 0;
+        }
+
+        if(flags & FAULT_FLAG_WRITE)
+                *page_table = huge_pte_mkdirty(*page_table);
+
+        *page_table = pte_mkyoung(*page_table);
+        *page_table = pte_unreserve(*page_table);
+
+        touch_page_addr = (void *)(address & PAGE_MASK);
+        ret = copy_from_user(&touched, (__force const void __user *)touch_page_addr, sizeof(unsigned long));
+
+	if(ret)
+		return VM_FAULT_SIGBUS;
+
+	/* Here where we do all our analysis */
+	current->total_dtlb_hugetlb_misses++;
+	current->total_dtlb_misses++;
+
+	*page_table = pte_mkreserve(*page_table);
+	return 0;
+}
+
 vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 			unsigned long address, unsigned int flags)
 {
@@ -3949,6 +4012,42 @@ vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 	unsigned long haddr = address & huge_page_mask(h);
 
 	ptep = huge_pte_offset(mm, haddr, huge_page_size(h));
+
+	mapping = vma->vm_file->f_mapping;
+	idx = vma_hugecache_offset(h, vma, haddr);
+
+	/*
+ 	 * Here we check for Huge page that are marked as reserved
+ 	 */
+	if(mm && mm->badger_trap_en && (!(flags & FAULT_FLAG_INSTRUCTION)) && ptep)
+	{
+		hash = hugetlb_fault_mutex_hash(h, mapping, idx, haddr);
+		mutex_lock(&hugetlb_fault_mutex_table[hash]);
+
+		entry = huge_ptep_get(ptep);
+		ptl = huge_pte_lock(h, mm, ptep);
+		if((flags & FAULT_FLAG_WRITE) && is_pte_reserved(entry) && !huge_pte_write(entry) && pte_present(entry))
+		{
+		        page = pte_page(entry);
+		        get_page(page);
+		        if (page != pagecache_page)
+		                lock_page(page);
+		        spin_lock(&mm->page_table_lock);
+			ret = hugetlb_cow(mm, vma, address, ptep, pagecache_page, ptl, flags);
+			goto out_put_page;
+		}
+		if(is_pte_reserved(entry) && pte_present(entry))
+		{
+			ret = hugetlb_fake_fault(mm, vma, address, ptep, flags);
+			goto out_mutex;
+		}
+		if(pte_present(entry))
+		{
+			*ptep = pte_mkreserve(*ptep);
+		}
+		mutex_unlock(&hugetlb_fault_mutex_table[hash]);
+	}
+
 	if (ptep) {
 		entry = huge_ptep_get(ptep);
 		if (unlikely(is_hugetlb_entry_migration(entry))) {
@@ -4036,7 +4135,7 @@ vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 	if (flags & FAULT_FLAG_WRITE) {
 		if (!huge_pte_write(entry)) {
 			ret = hugetlb_cow(mm, vma, address, ptep,
-					  pagecache_page, ptl);
+					  pagecache_page, ptl, flags);
 			goto out_put_page;
 		}
 		entry = huge_pte_mkdirty(entry);
diff --git a/mm/memory.c b/mm/memory.c
index e0010cb87..c2580ec05 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -93,6 +93,14 @@ struct page *mem_map;
 EXPORT_SYMBOL(mem_map);
 #endif
 
+extern inline pte_t pte_mkreserve(pte_t pte);
+extern inline pte_t pte_unreserve(pte_t pte);
+extern inline int is_pte_reserved(pte_t pte);
+extern inline pmd_t pmd_mkreserve(pmd_t pmd);
+extern inline pmd_t pmd_unreserve(pmd_t pmd);
+extern inline int is_pmd_reserved(pmd_t pmd);
+
+
 /*
  * A number of key systems in x86 including ioremap() rely on the assumption
  * that high_memory defines the upper bound on direct map memory, then end
@@ -2541,6 +2549,13 @@ static vm_fault_t wp_page_copy(struct vm_fault *vmf)
 		flush_cache_page(vma, vmf->address, pte_pfn(vmf->orig_pte));
 		entry = mk_pte(new_page, vma->vm_page_prot);
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+
+		/* Make the page table entry as reserved for TLB miss tracking */
+		if(mm && (mm->badger_trap_en==1) && (!(vmf->flags & FAULT_FLAG_INSTRUCTION)))
+		{
+			entry = pte_mkreserve(entry);
+		}
+			
 		/*
 		 * Clear the pte entry and flush it first, before updating the
 		 * pte with the new entry. This will avoid a race condition
@@ -3058,6 +3073,13 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 	flush_icache_page(vma, page);
 	if (pte_swp_soft_dirty(vmf->orig_pte))
 		pte = pte_mksoft_dirty(pte);
+
+	/* Make the page table entry as reserved for TLB miss tracking */
+	if(vma->vm_mm && (vma->vm_mm->badger_trap_en==1) && (!(vmf->flags & FAULT_FLAG_INSTRUCTION)))
+	{
+		pte = pte_mkreserve(pte);
+	}
+
 	set_pte_at(vma->vm_mm, vmf->address, vmf->pte, pte);
 	arch_do_swap_page(vma->vm_mm, vma, vmf->address, pte, vmf->orig_pte);
 	vmf->orig_pte = pte;
@@ -3216,6 +3238,12 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 	mem_cgroup_commit_charge(page, memcg, false, false);
 	lru_cache_add_active_or_unevictable(page, vma);
 setpte:
+	/* Make the page table entry as reserved for TLB miss tracking */
+	if(vma->vm_mm && (vma->vm_mm->badger_trap_en==1) && (!(vmf->flags & FAULT_FLAG_INSTRUCTION)))
+	{
+		entry = pte_mkreserve(entry);
+	}
+
 	set_pte_at(vma->vm_mm, vmf->address, vmf->pte, entry);
 
 	/* No need to invalidate - it was non-present before */
@@ -3287,6 +3315,58 @@ static vm_fault_t __do_fault(struct vm_fault *vmf)
 	return ret;
 }
 
+
+/*
+ * This function handles the fake page fault introduced to perform TLB miss
+ * studies. We can perform our work in this fuction on the page table entries.
+ */
+static int do_fake_page_fault(struct mm_struct *mm, struct vm_area_struct *vma,
+		unsigned long address, pte_t *page_table, pmd_t *pmd,
+		unsigned int flags, spinlock_t *ptl)
+{
+        unsigned long *touch_page_addr;
+        unsigned long touched;
+	unsigned long ret;
+	static unsigned int consecutive = 0;
+	static unsigned long prev_address = 0;
+
+	if(address == prev_address)
+		consecutive++;
+	else
+	{
+		consecutive = 0;
+		prev_address = address;
+	}
+
+	if(consecutive > 1)
+	{
+		*page_table = pte_unreserve(*page_table);
+		pte_unmap_unlock(page_table, ptl);
+		return 0;
+	}
+
+	if(flags & FAULT_FLAG_WRITE)
+		*page_table = pte_mkdirty(*page_table);
+
+	*page_table = pte_mkyoung(*page_table);
+	*page_table = pte_unreserve(*page_table);
+
+	touch_page_addr = (void *)(address & PAGE_MASK);
+	ret = __copy_from_user_inatomic(&touched, (__force const void __user *)touch_page_addr, sizeof(unsigned long));
+
+	if(ret)
+		return VM_FAULT_SIGBUS;
+
+	/* Here where we do all our analysis */
+	current->total_dtlb_4k_misses++;
+	current->total_dtlb_misses++;
+
+	*page_table = pte_mkreserve(*page_table);
+	pte_unmap_unlock(page_table, ptl);
+
+	return 0;
+}
+
 /*
  * The ordering of these checks is important for pmds with _PAGE_DEVMAP set.
  * If we check pmd_trans_unstable() first we will trip the bad_pmd() check
@@ -4052,6 +4132,37 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 	entry = vmf->orig_pte;
 	if (unlikely(!pte_same(*vmf->pte, entry)))
 		goto unlock;
+
+	if(vmf->vma->vm_mm && vmf->vma->vm_mm->badger_trap_en && (vmf->flags & FAULT_FLAG_INSTRUCTION)) {
+                if(vmf->pte && is_pte_reserved(*vmf->pte))
+                        *vmf->pte = pte_unreserve(*vmf->pte);	
+	}
+
+	/* We need to figure out if the page fault is a fake page fault or not.
+ 	 * If it is a fake page fault, we need to handle it specially. It has to
+ 	 * be made sure that the special page fault is not on instruction fault.
+ 	 * Our technique cannot not handle instruction page fault yet.
+ 	 *
+ 	 * We can have two cases when we have a fake page fault:
+ 	 * 1. We have taken a fake page fault on a COW page. A 
+ 	 * 	fake page fault on a COW page if for reading only
+ 	 * 	has to be considered a normal fake page fault. But
+ 	 * 	for writing purposes need to be handled correctly.
+ 	 * 2. We have taken a fake page fault on a normal page.
+ 	 */
+	if(vmf->vma->vm_mm && vmf->vma->vm_mm->badger_trap_en && (!(vmf->flags & FAULT_FLAG_INSTRUCTION)) && vmf->pte && pte_present(*vmf->pte))
+	{
+		if((vmf->flags & FAULT_FLAG_WRITE) && is_pte_reserved(entry) && !pte_write(entry))
+		{
+			return do_wp_page(vmf);
+		}
+		else if(is_pte_reserved(entry))
+		{
+			return do_fake_page_fault(vmf->vma->vm_mm, vmf->vma, vmf->address,
+						  vmf->pte, vmf->pmd, vmf->flags, vmf->ptl);
+		}
+	}
+
 	if (vmf->flags & FAULT_FLAG_WRITE) {
 		if (!pte_write(entry))
 			return do_wp_page(vmf);
@@ -4076,6 +4187,50 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 	return 0;
 }
 
+static int transparent_fake_fault(struct mm_struct *mm, struct vm_area_struct *vma,
+                        unsigned long address, pmd_t *page_table, unsigned int flags)
+{
+        unsigned long *touch_page_addr;
+        unsigned long touched;
+        unsigned long ret;
+        static unsigned int consecutive = 0;
+        static unsigned long prev_address = 0;
+
+        if(address == prev_address)
+                consecutive++;
+        else
+        {
+                consecutive = 0;
+                prev_address = address;
+        }
+
+        if(consecutive > 1)
+        {
+                *page_table = pmd_unreserve(*page_table);
+                return 0;
+        }
+
+        if(flags & FAULT_FLAG_WRITE)
+                *page_table = pmd_mkdirty(*page_table);
+
+        *page_table = pmd_mkyoung(*page_table);
+        *page_table = pmd_unreserve(*page_table);
+
+        touch_page_addr = (void *)(address & PAGE_MASK);
+        ret = __copy_from_user_inatomic(&touched, (__force const void __user *)touch_page_addr, sizeof(unsigned long));
+
+        if(ret)
+                return VM_FAULT_SIGBUS;
+
+        /* Here where we do all our analysis */
+        current->total_dtlb_hugetlb_misses++;
+        current->total_dtlb_misses++;
+
+        *page_table = pmd_mkreserve(*page_table);
+        return 0;
+}
+
+
 /*
  * By the time we get here, we already hold the mm semaphore
  *
@@ -4097,6 +4252,7 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 	pgd_t *pgd;
 	p4d_t *p4d;
 	vm_fault_t ret;
+	pmd_t entry;
 
 	pgd = pgd_offset(mm, address);
 	p4d = p4d_alloc(mm, pgd, address);
@@ -4106,6 +4262,7 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 	vmf.pud = pud_alloc(mm, p4d, address);
 	if (!vmf.pud)
 		return VM_FAULT_OOM;
+
 	if (pud_none(*vmf.pud) && transparent_hugepage_enabled(vma)) {
 		ret = create_huge_pud(&vmf);
 		if (!(ret & VM_FAULT_FALLBACK))
@@ -4132,6 +4289,40 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 	vmf.pmd = pmd_alloc(mm, vmf.pud, address);
 	if (!vmf.pmd)
 		return VM_FAULT_OOM;
+
+        /*
+ 	 * Here we check for transparent huge page that are marked as reserved
+         */
+        if(mm && mm->badger_trap_en && (!(flags & FAULT_FLAG_INSTRUCTION)) && !pmd_none(*vmf.pmd) && pmd_trans_huge(*vmf.pmd))
+        {
+                spin_lock(&mm->page_table_lock);
+                entry = *vmf.pmd;
+                if((flags & FAULT_FLAG_WRITE) && is_pmd_reserved(entry) && !pmd_write(entry) && pmd_present(entry))
+                {
+                        spin_unlock(&mm->page_table_lock);
+			return do_huge_pmd_wp_page(&vmf, entry);
+                }
+                if(is_pmd_reserved(entry) && pmd_present(entry))
+                {
+                        ret = transparent_fake_fault(mm, vma, address, vmf.pmd, flags);
+                        spin_unlock(&mm->page_table_lock);
+			return ret;
+                }
+		/* FIXME: splitting dropped? */
+#if 0
+		if(pmd_present(entry) && pmd_trans_splitting(entry))
+		{
+			spin_unlock(&mm->page_table_lock);
+			goto escape;
+		}
+#endif
+                if(pmd_present(entry))
+                {
+			*vmf.pmd = pmd_mkreserve(*vmf.pmd);
+                }
+		spin_unlock(&mm->page_table_lock);
+	}
+
 	if (pmd_none(*vmf.pmd) && transparent_hugepage_enabled(vma)) {
 		ret = create_huge_pmd(&vmf);
 		if (!(ret & VM_FAULT_FALLBACK))
@@ -4140,6 +4331,7 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 		pmd_t orig_pmd = *vmf.pmd;
 
 		barrier();
+
 		if (unlikely(is_swap_pmd(orig_pmd))) {
 			VM_BUG_ON(thp_migration_supported() &&
 					  !is_pmd_migration_entry(orig_pmd));
@@ -4147,6 +4339,7 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 				pmd_migration_entry_wait(mm, vmf.pmd);
 			return 0;
 		}
+
 		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
 			if (pmd_protnone(orig_pmd) && vma_is_accessible(vma))
 				return do_huge_pmd_numa_page(&vmf, orig_pmd);
